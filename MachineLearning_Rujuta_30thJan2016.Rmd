---
title: "Rujuta_Coursera_Machine_Learning_Project_Jan16"
author: "Rujuta Joshi"
date: "Thursday, January 28, 2016"
output: html_document
---

#Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

# Methodology 

# Step 1 : downloading the test data and the train data.

```{r}

#traindataurl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
#download.file(traindataurl, destfile="./TrainData.csv")
TrainData <- read.csv("./TrainData.csv", header =TRUE)
dim(TrainData)
# understand a little bit about the Train Data
names(TrainData)
# for cross validation, i need to make a part of my data, for training and testing, and then use
# the test data only for running the final model. 

#testdataurl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
#download.file(testdataurl, destfile="./TestData.csv")
TestData <- read.csv("./TestData.csv", header=TRUE)

dim(TestData)

```

# Step 2 : Exploratory Analysis of the data, to understand it. , here we also make sure all relevant packages are installed and opened. 

```{r}
# Exploratory Analysis 

str(TrainData)
# understand the missing values 
na_count2 <- sapply(TrainData, function(y) sum(length(which(is.na(y)))))
str(na_count2)

library(caret)
library(rpart)
library(rpart.plot)
library(RColorBrewer)

library(randomForest)
library(knitr)


```


# Steps in Data Preparation 
The following steps are taken. 
1. we need to predict values of the 20 test data points. However in order to train the model that we build, we need another test data, where we can check the accuracy. so we create a partition in the data, and create another validation data set, called Test set, where we can check how well is the model doing. 
2. We make sure that the NA columns are removed, the data is in the same format in these 3 sets, and remove the first column. 



# 1. making the partition in Training Data - training and validation data sets
```{r, cache=TRUE}
Trainpart <- createDataPartition (TrainData$classe, p=0.7, list=FALSE)
TrainingSubset <- TrainData[Trainpart,]
TestSubset <- TrainData[-Trainpart,]
dim(TrainingSubset)
dim(TestSubset)
```

# 2. Clean the data by  a] Clearing near zero variance variables, b] Clearing empty columns

```{r, cache=TRUE}
nz <- nearZeroVar(TrainingSubset, saveMetrics=TRUE)
dim(nz)
myTraining <- TrainingSubset[,nz$nzv==FALSE]
dim(myTraining)

nz2 <- nearZeroVar(TestSubset, saveMetrics=TRUE)
dim(nz2)
myTest <- TestSubset[, nz2$nzv==FALSE]
dim(myTest)

#2b. Removing the first column 
myTraining <- myTraining [c(-1)]
#2c  Remove the columns that have mostly NA- Removing for 60%

trainingroughwork <- myTraining

for (i in 1: length(myTraining))
{if (sum(is.na(myTraining[,i]))/nrow(myTraining) >= 0.6)
{for(j in 1: length(trainingroughwork))
{if (length(grep(names(myTraining[i]),
names(trainingroughwork)[j]))==1)
{trainingroughwork <- trainingroughwork[,-j]}
}

}
}

myTraining <- trainingroughwork

#2d now do this procedure for the myTest data(validation set ) and the testing data
smallsetcolumnnames <- colnames(myTraining)
smallsetcolname2 <- colnames(myTraining[,-58])

myTest <- myTest[smallsetcolumnnames]
TestData <- TestData[smallsetcolname2]

# 2e - make sure that the data in Training set, validation set and the test set is in the same format




for (i in 1:length(TestData) ) {
  for(j in 1:length(myTraining)) {
    if( length( grep(names(myTraining[i]), names(TestData)[j]) ) == 1)  {
      class(TestData[j]) <- class(myTraining[i])
    }      
  }      
}

# To get the same class between TestData and myTraining
TestData <- rbind(myTraining[2, -58] , TestData)
TestData <- TestData[-1,]

```



# Step 3 : Prediction using Decision Trees and Random Forest and Generalised boosting regression. 
we check with all 3 methods, check which one has the best accuracy and go ahead with the model for that method. for this data, it turns out that random forest works best for us. 

```{r, cache=TRUE}

# Prediction using Decision Tree Analysis

set.seed(11111)
library(rpart)
library(rpart.plot)
library(rattle)
ModelFitA1 <- rpart(classe~.,data=myTraining,method="class")

predictionsA1 <- predict(ModelFitA1, myTest, type = "class")
cmtree <- confusionMatrix(predictionsA1, myTest$classe)
cmtree

plot(cmtree$table, col = cmtree$byClass, main = 
       paste("Decision Tree Confusion Matrix: Accuracy =", 
             round(cmtree$overall['Accuracy'], 4)))
# Prediction with Random Forests

set.seed(22222)
library(randomForest)
ModelFitB1 <- randomForest(classe~., data=myTraining)
predictionB1 <- predict(ModelFitB1, myTest, type="class")
cmrf <- confusionMatrix(predictionB1, myTest$classe)
cmrf
plot(ModelFitB1)
plot(cmrf$table, col=cmtree$byClass, 
     main=paste("Random Forest Confusion Matrix: 
                Accuracy =", round(cmrf$overall['Accuracy'],4)))


# Method 3 : Prediction with Generalised Boosted Regression 

set.seed(33333)
fitControl <- trainControl(method = "repeatedcv",
                           number = 5,
                           repeats = 1)

gbmFit1 <- train(classe ~ ., data=myTraining, method = "gbm",
                 trControl = fitControl,
                 verbose = FALSE)


gbmFinMod1 <- gbmFit1$finalModel

gbmPredTest <- predict(gbmFit1, newdata=myTest)
gbmAccuracyTest <- confusionMatrix(gbmPredTest, myTest$classe)
gbmAccuracyTest

plot(gbmFit1, ylim=c(0.9,1))

```

#Step 4 : Now that we know that we have high accuracy on the validation data set, lets predict for the test data- Thus here are the results. 

```{r, cache=TRUE}

PredictedResults <- predict(ModelFitB1, TestData, type="class")
PredictedResults
```



